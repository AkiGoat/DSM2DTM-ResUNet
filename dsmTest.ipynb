{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-unet' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(\"pytorch_unet.py\"):\n",
    "    if not os.path.exists(\"pytorch_unet\"):\n",
    "        !git clone https://github.com/akigoat/pytorch-unet.git\n",
    "\n",
    "    # %cd pytorch-unet\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import math\n",
    "# files\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "# matplot\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import colors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# GIS\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "import richdem as rd\n",
    "from rasterio.fill import fillnodata\n",
    "# torch and dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# functions\n",
    "import sys\n",
    "sys.path.append('pytorch-unet')\n",
    "import helper\n",
    "\n",
    "import dataFunctions\n",
    "import netClasses\n",
    "\n",
    "try:\n",
    "    device = torch.device(\"mps\")\n",
    "except:\n",
    "    print(\"Check device\")\n",
    "    print(\"available: \", torch.backends.mps.is_available())\n",
    "    print(\"built: \", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.90327775, 2.48690886, 1.35894729])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([19.5961904 ,  2.92635182,  1.85182111])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_folder = '/Volumes/HydesT7/Grad Project/Data/U_Net/64_64_1_8_reselected/'\n",
    "# path_folder = '/Users/hyde-mbp/Projects/Grad/data/64_64_1_8_rebuild/'\n",
    "# path_model_folder = '/Users/hyde-mbp/Projects/Grad/model/0330/'\n",
    "# path_model_folder = '/Volumes/HydesT7/Grad Project/Model/U_Net/1013/'\n",
    "path_model_folder = '/Users/hyde-mbp/Projects/Grad/model/0405/'\n",
    "path_ANH4 = '/Volumes/HydesT7/Grad Project/Data/AHN4_all/'\n",
    "# path_pre = '/Volumes/HydesT7/Grad Project/Data/AHN4_preprocessed_1_8/'\n",
    "\n",
    "means = np.load(path_folder+'para_means.npy')\n",
    "stds = np.load(path_folder+'para_stds.npy')\n",
    "\n",
    "# use the same transformations for train/val in this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([means[0], means[1], means[2]], [\n",
    "                         stds[0], stds[1], stds[2]])  # imagenet\n",
    "])\n",
    "\n",
    "list_patch = []\n",
    "with open(path_ANH4+'test_patchs.txt') as fp:\n",
    "    # list_patch.append(patch)\n",
    "    list_patch = [line.rstrip('\\n') for line in fp]\n",
    "\n",
    "display(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeImg(im_list, im_dict, mask=False):\n",
    "    height = int(im_dict[list(im_dict)[-1]].split(',')[0].split(':')[1])\n",
    "    width = int(im_dict[list(im_dict)[-1]].split(',')[1].split(':')[1])\n",
    "    im_merged = np.zeros((height, width), dtype=np.float32)\n",
    "    if not mask:\n",
    "        ndv = 3.4028230607370965e+38\n",
    "        im_merged = im_merged-ndv\n",
    "    else:\n",
    "        im_merged = im_merged+0.5\n",
    "    for k in im_dict:\n",
    "        h0 = int(im_dict[k].split(',')[0].split(':')[0])\n",
    "        h1 = int(im_dict[k].split(',')[0].split(':')[1])\n",
    "        w0 = int(im_dict[k].split(',')[1].split(':')[0])\n",
    "        w1 = int(im_dict[k].split(',')[1].split(':')[1])\n",
    "        if not mask:\n",
    "            im_merged[h0:h1, w0:w1] = np.where(\n",
    "                im_list[k] > im_merged[h0:h1, w0:w1], im_list[k], im_merged[h0:h1, w0:w1])\n",
    "        else:\n",
    "            im_merged[h0:h1, w0:w1] = np.where(abs(\n",
    "                im_list[k]-0.5) > abs(im_merged[h0:h1, w0:w1]-0.5), im_list[k], im_merged[h0:h1, w0:w1])\n",
    "    return im_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyde-mbp/mambaforge/envs/torG2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hyde-mbp/mambaforge/envs/torG2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetUNet(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer0_1x1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer1_1x1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2_1x1): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3_1x1): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4_1x1): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (conv_up3): Sequential(\n",
       "    (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up2): Sequential(\n",
       "    (0): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up1): Sequential(\n",
       "    (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_up0): Sequential(\n",
       "    (0): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_original_size2): Sequential(\n",
       "    (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_last): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = netClasses.ResNetUNet(2)\n",
    "model = model.to(device)\n",
    "# /Users/hyde-mbp/Projects/Grad/model/0330/dsm_rou_slo_30_1e-4.pt\n",
    "model.load_state_dict(torch.load(path_model_folder+'30_1e-4.pt'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsm2dtm(dsm):\n",
    "    tmp_path = 'data/tmpp/'\n",
    "    cut_height = 64\n",
    "    cut_width = 64\n",
    "\n",
    "    Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(tmp_path):\n",
    "        file_path = os.path.join(tmp_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    with rasterio.open(dsm) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1),\n",
    "                                smoothing_iterations=0)\n",
    "\n",
    "    dsmf = tmp_path+'filledDSM.TIF'\n",
    "    with rasterio.open(dsmf, 'w', **profile) as dest:\n",
    "        dest.write_band(1, arr_filled)\n",
    "\n",
    "    slope_ = tmp_path+'slope.TIF'\n",
    "    _ = gdal.DEMProcessing(slope_, dsmf, 'slope', computeEdges=True)\n",
    "    _ = None\n",
    "    # if checkNodata(slope_):\n",
    "    #     list_hasnodata.append(slope_)\n",
    "\n",
    "    roughness_ = tmp_path+'Roughness.TIF'\n",
    "    _ = gdal.DEMProcessing(roughness_, dsmf, 'Roughness', computeEdges=True)\n",
    "    _ = None\n",
    "\n",
    "    dsm_fill_slices, dsm_dict = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'filledDSM.TIF'), cut_height, cut_width, dt=True)\n",
    "    slope_slices = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'slope.TIF'), cut_height, cut_width)\n",
    "    roughness_slices = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'Roughness.TIF'), cut_height, cut_width)\n",
    "    \n",
    "    Imgs = dataFunctions.stackPics(dsm_fill_slices, roughness_slices, slope_slices)\n",
    "\n",
    "    test_dataset = netClasses.TestDataset2(Imgs, transform=trans)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Imgs.shape[0],\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "\n",
    "    pred = model(inputs)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.data.cpu().numpy()\n",
    "    threshold, upper, lower = 0.5, 1, 0\n",
    "    pred = np.where(pred > threshold, upper, lower)\n",
    "\n",
    "    pred_l1 = []\n",
    "    for p in list(pred):\n",
    "        pred_l1.append(p[1])\n",
    "    pred_w = mergeImg(pred_l1, dsm_dict, mask=True)\n",
    "\n",
    "    dtm_re = rd.LoadGDAL(dsm)\n",
    "\n",
    "    for i in range(pred_w.shape[0]):\n",
    "        for j in range(pred_w.shape[1]):\n",
    "            if pred_w[i][j] > 0.5:\n",
    "                dtm_re[i][j] = dtm_re.no_data\n",
    "                # print(dsm_rd[i][j])\n",
    "\n",
    "\n",
    "    return dtm_re, pred_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorAnalysis(dsm, dtm, padding=False):\n",
    "    tmp_path = 'data/tmpp/'\n",
    "    cut_height = 64\n",
    "    cut_width = 64\n",
    "\n",
    "\n",
    "    Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(tmp_path):\n",
    "        file_path = os.path.join(tmp_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    _ = shutil.copyfile(dsm, os.path.join(tmp_path, 'DSM.TIF'))\n",
    "    _ = shutil.copyfile(dtm, os.path.join(tmp_path, 'DTM.TIF'))\n",
    "    \n",
    "    with rasterio.open(dsm) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1),\n",
    "                                smoothing_iterations=0)\n",
    "\n",
    "    dsmf = tmp_path+'filledDSM.TIF'\n",
    "    with rasterio.open(dsmf, 'w', **profile) as dest:\n",
    "        dest.write_band(1, arr_filled)\n",
    "\n",
    "    slope_ = tmp_path+'slope.TIF'\n",
    "    _ = gdal.DEMProcessing(slope_, dsmf, 'slope', computeEdges=True)\n",
    "    _ = None\n",
    "    # if checkNodata(slope_):\n",
    "    #     list_hasnodata.append(slope_)\n",
    "\n",
    "    roughness_ = tmp_path+'Roughness.TIF'\n",
    "    _ = gdal.DEMProcessing(roughness_, dsmf, 'Roughness', computeEdges=True)\n",
    "    _ = None\n",
    "\n",
    "    dsm_fill_slices, dsm_dict = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'filledDSM.TIF', padding=padding), cut_height, cut_width, dt=True)\n",
    "    slope_slices = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'slope.TIF', padding=padding), cut_height, cut_width)\n",
    "    roughness_slices = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'Roughness.TIF', padding=padding), cut_height, cut_width)\n",
    "\n",
    "    dsm_slices = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'DSM.TIF', masked=True, padding=padding), cut_width, cut_height)\n",
    "    dtm_slices = dataFunctions.sliceRisPic(dataFunctions.readTIFF(tmp_path, 'DTM.TIF', masked=True, padding=padding), cut_width, cut_height)\n",
    "    \n",
    "    Imgs = dataFunctions.stackPics(dsm_fill_slices, roughness_slices, slope_slices)\n",
    "\n",
    "    ground_truth_slices = []\n",
    "    for nn in range(len(dsm_slices)):\n",
    "        ground_truth_slices.append(dataFunctions.generateGroundTruth(dsm_slices[nn], dtm_slices[nn], 0.5))\n",
    "    \n",
    "    Masks = np.stack(ground_truth_slices)\n",
    "    Masks = Masks.astype(np.float32)\n",
    "    \n",
    "    gt = []\n",
    "    for mask in ground_truth_slices:\n",
    "        gt.append(mask[1])\n",
    "    gt_w = mergeImg(gt, dsm_dict, mask=True)\n",
    "    \n",
    "    test_dataset = netClasses.TestDataset2(Imgs, transform=trans)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Imgs.shape[0],\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "\n",
    "    pred = model(inputs)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.data.cpu().numpy()\n",
    "    threshold, upper, lower = 0.5, 1, 0\n",
    "    pred = np.where(pred > threshold, upper, lower)\n",
    "\n",
    "    pred_l1 = []\n",
    "    for p in list(pred):\n",
    "        pred_l1.append(p[1])\n",
    "    pred_w = mergeImg(pred_l1, dsm_dict, mask=True)\n",
    "\n",
    "    dtm_re = rd.LoadGDAL(dsm)\n",
    "\n",
    "    for i in range(dtm_re.shape[0]):\n",
    "        for j in range(dtm_re.shape[1]):\n",
    "            if pred_w[i][j] > 0.5:\n",
    "                dtm_re[i][j] = dtm_re.no_data\n",
    "                # print(dsm_rd[i][j])\n",
    "\n",
    "    dsm_ = dataFunctions.readTIFF(tmp_path, 'DSM.TIF', masked=True)\n",
    "    dtm_ = dataFunctions.readTIFF(tmp_path, 'DTM.TIF', masked=True)\n",
    "\n",
    "    Ndsm = dsm_.mask == True\n",
    "    Ndtm = dtm_.mask == True\n",
    "\n",
    "    mask_error = np.abs(pred_w - gt_w) > 0.1\n",
    "    error_count = np.sum(mask_error)\n",
    "    # error_wrong_judge = np.sum(mask_error & Ndsm)\n",
    "    error_wrong_judge = np.sum(mask_error & Ndtm)\n",
    "    error_ele = np.sum(mask_error & ~Ndsm & ~Ndtm)\n",
    "    # bias_ele = (dsm_.data - dtm_.data)[mask_error & ~Ndsm & ~Ndtm]\n",
    "\n",
    "    acc_count = np.sum(~mask_error)\n",
    "\n",
    "    dic_ = {'acc': acc_count/(pred_w.shape[0]*pred_w.shape[1]), \n",
    "            'error_count': error_count, \n",
    "            'error_wrong_judge': error_wrong_judge,  \n",
    "            'error_ele': error_ele}\n",
    "\n",
    "    return pred_w, gt_w, dtm_re, dic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffRes(path_ANH4, name, folder_out, res=30, padding=False):\n",
    "    Path(folder_out).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(folder_out, str(res))).mkdir(parents=True, exist_ok=True)\n",
    "    tmp_path = 'data/tmppp/'\n",
    "    Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    xres = res\n",
    "    yres = res\n",
    "    resample_alg = 'average'\n",
    "\n",
    "    dsm = path_ANH4+'R5_'+name+'.TIF'\n",
    "    dtm = path_ANH4+'M5_'+name+'.TIF'\n",
    "\n",
    "    dsmre = tmp_path+'R'+str(res)+'_'+name+'.TIF'\n",
    "    _ = gdal.Warp(dsmre, dsm, xRes=xres, yRes=yres, resampleAlg=resample_alg)\n",
    "    _ = None\n",
    "\n",
    "    dtmre = tmp_path+'M'+str(res)+'_'+name+'.TIF'\n",
    "    _ = gdal.Warp(dtmre, dtm, xRes=xres, yRes=yres, resampleAlg=resample_alg)\n",
    "    _ = None    \n",
    "    \n",
    "    pred_w, gt_w, dtm_re, dic_ = errorAnalysis(dsmre, dtmre, padding=padding)\n",
    "    rdarr_dsmre = rd.LoadGDAL(dsmre)\n",
    "    pred_w_ = pred_w[0:rdarr_dsmre.shape[0], 0:rdarr_dsmre.shape[1]]\n",
    "    gt_w_ = gt_w[0:rdarr_dsmre.shape[0], 0:rdarr_dsmre.shape[1]]\n",
    "\n",
    "    rd.SaveGDAL(os.path.join(tmp_path, name+'_DTM.TIF'), dtm_re)\n",
    "    with rasterio.open(os.path.join(tmp_path, name+'_DTM.TIF')) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1), smoothing_iterations=0)\n",
    "    dtmref = os.path.join(tmp_path, name+'_DTM_filled.TIF')  \n",
    "    with rasterio.open(dtmref, 'w', **profile) as dest:\n",
    "        dest.write_band(1, arr_filled)\n",
    "\n",
    "    with rasterio.open(dsmre) as src_:\n",
    "        profile_ = src_.profile\n",
    "        arr_ = src_.read(1)\n",
    "        arr_filled_ = fillnodata(arr_, mask=src_.read_masks(1), smoothing_iterations=0)\n",
    "    dsmref = os.path.join(tmp_path, name+'_DSM_filled.TIF')  \n",
    "    with rasterio.open(dsmref, 'w', **profile_) as dest_:\n",
    "        dest_.write_band(1, arr_filled_)\n",
    "    \n",
    "    with rasterio.open(dtmre) as src_:\n",
    "        profile_ = src_.profile\n",
    "        arr_ = src_.read(1)\n",
    "        arr_filled_ = fillnodata(arr_, mask=src_.read_masks(1), smoothing_iterations=0)\n",
    "    dtmref = os.path.join(tmp_path, name+'_DTM_filled.TIF')  \n",
    "    with rasterio.open(dtmref, 'w', **profile_) as dest_:\n",
    "        dest_.write_band(1, arr_filled_)    \n",
    "    \n",
    "    \n",
    "\n",
    "    return dtm_re\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 56)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = diffRes(path_ANH4=path_ANH4, name='10EN2', folder_out='result_otherRes', res=90, padding=True)\n",
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDtmNPlots(cop, fabdem, name, folder_out):\n",
    "    Path(folder_out).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dtm_re, pred_w = dsm2dtm(cop)\n",
    "\n",
    "    rd.SaveGDAL(os.path.join(folder_out, name+'_DTM.TIF'), dtm_re)\n",
    "    with rasterio.open(os.path.join(folder_out, name+'_DTM.TIF')) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1), smoothing_iterations=0)\n",
    "    dtmf = os.path.join(folder_out, name+'_DTM_filled.TIF')  \n",
    "    with rasterio.open(dtmf, 'w', **profile) as dest:\n",
    "        dest.write_band(1, arr_filled)\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    rda_cop = rd.LoadGDAL(cop)\n",
    "    disparr_cop = np.array(rda_cop, copy=True)\n",
    "    disparr_cop[disparr_cop < rda_cop.no_data/10] = np.nan\n",
    "    vmin, vmax = np.nanpercentile(disparr_cop, [2, 98])\n",
    "    rda_fabdem = rd.LoadGDAL(fabdem)\n",
    "    disparr_fabdem = np.array(rda_fabdem, copy=True)\n",
    "    disparr_fabdem[disparr_fabdem < rda_fabdem.no_data/10] = np.nan\n",
    "    rda_dtmf = rd.LoadGDAL(dtmf)\n",
    "    disparr_dtmf = np.array(rda_dtmf, copy=True)\n",
    "    disparr_dtmf[disparr_dtmf < rda_dtmf.no_data/10] = np.nan\n",
    "\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    im1 = ax1.imshow(disparr_cop, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax1.set_xlim()\n",
    "    ax1.set_ylim()\n",
    "    ax1.set_title('COP30')\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im1, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    im2 = ax2.imshow(disparr_fabdem, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax2.set_xlim()\n",
    "    ax2.set_ylim()\n",
    "    ax2.set_title('FABDEM')\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im2, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    im3 = ax3.imshow(disparr_dtmf, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax3.set_xlim()\n",
    "    ax3.set_ylim()\n",
    "    ax3.set_title('DTM generated by the model')\n",
    "    divider = make_axes_locatable(ax3)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im3, cax=cax, orientation='vertical')\n",
    "\n",
    "    color = [[255, 197, 77], [83, 191, 157]]\n",
    "    color = np.array(color)/255\n",
    "    cmap = colors.ListedColormap(color)\n",
    "    norm = colors.BoundaryNorm([0, 0.5, 1], 2, clip=True)\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    im4 = ax4.imshow(pred_w, cmap=cmap, norm=norm)\n",
    "    ax4.set_title('Prediction')\n",
    "    divider = make_axes_locatable(ax4)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im4, cax=cax, orientation='vertical', ticks=np.linspace(0, 1, 2))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(folder_out, name+'_f4.png'))\n",
    "\n",
    "    extent1 = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(folder_out, name+'_COP30.png'), bbox_inches=extent1.expanded(1.3, 1.1))\n",
    "    extent2 = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(folder_out, name+'_FABDEM.png'), bbox_inches=extent2.expanded(1.3, 1.1))\n",
    "    extent3 = ax3.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(folder_out, name+'_DTMf.png'), bbox_inches=extent3.expanded(1.3, 1.1))\n",
    "    extent4 = ax4.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(folder_out, name+'_Pred.png'), bbox_inches=extent4.expanded(1.3, 1.1))\n",
    "\n",
    "    plt.close()\n",
    "    # rd.SaveGDAL(os.path.join(folder_out, name+'_DTM.TIF'), dtm_re)\n",
    "\n",
    "    # with rasterio.open(os.path.join(folder_out, name+'_DTM.TIF')) as src:\n",
    "    #     profile = src.profile\n",
    "    #     arr = src.read(1)\n",
    "    #     arr_filled = fillnodata(arr, mask=src.read_masks(1), smoothing_iterations=0)\n",
    "\n",
    "    # # dtm30ref = tmp_path+'M30_'+item+'_re_filled.TIF'  \n",
    "    # with rasterio.open(os.path.join(folder_out, name+'_DTM_filled.TIF'), 'w', **profile) as dest:\n",
    "    #     dest.write_band(1, arr_filled)\n",
    "\n",
    "\n",
    "    return f'Plots are saved as {folder_out}/{name}_f4.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rda2arr(rda):\n",
    "    no_data = rda.no_data\n",
    "    arr = np.array(rda, copy=True)\n",
    "    if no_data>0:\n",
    "        arr[arr>no_data/10] = np.nan\n",
    "    else:\n",
    "        arr[arr<no_data/10] = np.nan\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmseNMore(path_cop, path_fabdem, path_ANH4, name, folder_out):\n",
    "    Path(folder_out).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(folder_out, 'cop')).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(folder_out, 'AHN4')).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(folder_out, 'fabdem')).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(folder_out, 'pred')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tmp_path = 'data/tmp/'\n",
    "    xres = 30\n",
    "    yres = 30\n",
    "    resample_alg = 'average'\n",
    "    # cut_height = 64\n",
    "    # cut_width = 64\n",
    "\n",
    "    Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(tmp_path):\n",
    "        file_path = os.path.join(tmp_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    cop = os.path.join(path_cop, name+'.tiff')\n",
    "    fabdem = os.path.join(path_fabdem, name+'.tiff')\n",
    "\n",
    "    dsm5 = path_ANH4+'R5_'+name+'.TIF'\n",
    "    dtm5 = path_ANH4+'M5_'+name+'.TIF'\n",
    "\n",
    "    dsm30 = tmp_path+'R30_'+name+'.TIF'\n",
    "    _ = gdal.Warp(dsm30, dsm5, xRes=xres, yRes=yres, resampleAlg=resample_alg)\n",
    "    _ = None\n",
    "\n",
    "    with rasterio.open(dsm30) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1),\n",
    "                                smoothing_iterations=0)\n",
    "\n",
    "    dsm30f = tmp_path+'R30_'+name+'_filled.TIF'\n",
    "    with rasterio.open(dsm30f, 'w', **profile) as dest:\n",
    "        dest.write_band(1, arr_filled)\n",
    "\n",
    "\n",
    "    dtm30 = tmp_path+'M30_'+name+'.TIF'\n",
    "    _ = gdal.Warp(dtm30, dtm5, xRes=xres, yRes=yres, resampleAlg=resample_alg)\n",
    "    _ = None\n",
    "\n",
    "    with rasterio.open(dtm30) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1),\n",
    "                                smoothing_iterations=0)\n",
    "\n",
    "    dtm30f = tmp_path+'M30_'+name+'_filled.TIF'\n",
    "    with rasterio.open(dtm30f, 'w', **profile) as dest_:\n",
    "        dest_.write_band(1, arr_filled)\n",
    "    \n",
    "\n",
    "    dtm_A, pred_A = dsm2dtm(dsm30f)\n",
    "    dtm_cop, pred_cop = dsm2dtm(cop)\n",
    "\n",
    "    rd.SaveGDAL(os.path.join(folder_out, name+'_DTM.TIF'), dtm_A)\n",
    "    with rasterio.open(os.path.join(folder_out, name+'_DTM.TIF')) as src:\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)\n",
    "        arr_filled = fillnodata(arr, mask=src.read_masks(1), smoothing_iterations=0)\n",
    "    dtmf = os.path.join(folder_out, name+'_DTM_filled.TIF')  \n",
    "    with rasterio.open(dtmf, 'w', **profile) as dest:\n",
    "        dest.write_band(1, arr_filled)\n",
    "\n",
    "    rd.SaveGDAL(os.path.join(folder_out, name+'_DTM_COP.TIF'), dtm_cop)\n",
    "    with rasterio.open(os.path.join(folder_out, name+'_DTM_COP.TIF')) as src_:\n",
    "        profile_ = src_.profile\n",
    "        arr_ = src_.read(1)\n",
    "        arr_filled_ = fillnodata(arr_, mask=src_.read_masks(1), smoothing_iterations=0)\n",
    "    dtmfc = os.path.join(folder_out, name+'_DTM_COP_filled.TIF')  \n",
    "    with rasterio.open(dtmfc, 'w', **profile_) as dest_:\n",
    "        dest_.write_band(1, arr_filled_)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 24))\n",
    "\n",
    "    # rda_cop = rd.LoadGDAL(cop)\n",
    "    # disparr_cop = np.array(rda_cop, copy=True)\n",
    "    # disparr_cop[disparr_cop < rda_cop.no_data/10] = np.nan\n",
    "    narr_cop = rda2arr(rd.LoadGDAL(cop))\n",
    "    # vmin, vmax = np.nanpercentile(narr_cop, [2, 98])\n",
    "    narr_dsmf = rda2arr(rd.LoadGDAL(dsm30f))\n",
    "    vmin, vmax = np.nanpercentile(np.concatenate((narr_cop.ravel(), narr_dsmf.ravel())), [2, 98])\n",
    "    narr_dtmf = rda2arr(rd.LoadGDAL(dtmf))\n",
    "    narr_dtmfc = rda2arr(rd.LoadGDAL(dtmfc))\n",
    "    narr_dtmAf = rda2arr(rd.LoadGDAL(dtm30f))\n",
    "    narr_fabdem = rda2arr(rd.LoadGDAL(fabdem))\n",
    "\n",
    "    ax1 = fig.add_subplot(421)\n",
    "    im1 = ax1.imshow(narr_dsmf, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax1.set_xlim()\n",
    "    ax1.set_ylim()\n",
    "    ax1.set_title('AHN4 DSM (resampled to 30m)')\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im1, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax2 = fig.add_subplot(422)\n",
    "    im2 = ax2.imshow(narr_dtmf, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax2.set_xlim()\n",
    "    ax2.set_ylim()\n",
    "    ax2.set_title('DTM generated by the model')\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im2, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax3 = fig.add_subplot(423)\n",
    "    im3 = ax3.imshow(narr_cop, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax3.set_xlim()\n",
    "    ax3.set_ylim()\n",
    "    ax3.set_title('COP30')\n",
    "    divider = make_axes_locatable(ax3)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im3, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax4 = fig.add_subplot(424)\n",
    "    im4 = ax4.imshow(narr_dtmfc, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax4.set_xlim()\n",
    "    ax4.set_ylim()\n",
    "    ax4.set_title('DTM generated by the model')\n",
    "    divider = make_axes_locatable(ax4)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im4, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax5 = fig.add_subplot(425)\n",
    "    im5 = ax5.imshow(narr_dtmAf, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax5.set_xlim()\n",
    "    ax5.set_ylim()\n",
    "    ax5.set_title('AHN4 DTM (resampled to 30m)')\n",
    "    divider = make_axes_locatable(ax5)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im5, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax6 = fig.add_subplot(426)\n",
    "    im6 = ax6.imshow(narr_fabdem, vmin=vmin, vmax=vmax, cmap='terrain')\n",
    "    ax6.set_xlim()\n",
    "    ax6.set_ylim()\n",
    "    ax6.set_title('FABDEM')\n",
    "    divider = make_axes_locatable(ax6)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im6, cax=cax, orientation='vertical')\n",
    "\n",
    "    color = [[255, 197, 77], [83, 191, 157]]\n",
    "    color = np.array(color)/255\n",
    "    cmap = colors.ListedColormap(color)\n",
    "    norm = colors.BoundaryNorm([0, 0.5, 1], 2, clip=True)\n",
    "\n",
    "    ax7 = fig.add_subplot(427)\n",
    "    im7 = ax7.imshow(pred_A, cmap=cmap, norm=norm)\n",
    "    ax7.set_title('Prediction of AHN4 DSM')\n",
    "    divider = make_axes_locatable(ax7)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im7, cax=cax, orientation='vertical', ticks=np.linspace(0, 1, 2))\n",
    "\n",
    "    ax8 = fig.add_subplot(428)\n",
    "    im8 = ax8.imshow(pred_cop, cmap=cmap, norm=norm)\n",
    "    ax8.set_title('Prediction of COP30')\n",
    "    divider = make_axes_locatable(ax8)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im8, cax=cax, orientation='vertical', ticks=np.linspace(0, 1, 2))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(folder_out, name+'_f8.png'))\n",
    "\n",
    "    extent1 = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'AHN4'), name+'_DSM.png'), bbox_inches=extent1.expanded(1.3, 1.1))\n",
    "    extent2 = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'AHN4'), name+'_DTMg.png'), bbox_inches=extent2.expanded(1.3, 1.1))\n",
    "    extent3 = ax3.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'cop'), name+'_COP30.png'), bbox_inches=extent3.expanded(1.3, 1.1))\n",
    "    extent4 = ax4.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'cop'), name+'_copDTM.png'), bbox_inches=extent4.expanded(1.3, 1.1))   \n",
    "    extent5 = ax5.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'AHN4'), name+'_DTMo.png'), bbox_inches=extent5.expanded(1.3, 1.1))\n",
    "    extent6 = ax6.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'fabdem'), name+'_fab.png'), bbox_inches=extent6.expanded(1.3, 1.1))\n",
    "    extent7 = ax7.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'pred'), name+'_AHN4.png'), bbox_inches=extent7.expanded(1.3, 1.1))\n",
    "    extent8 = ax8.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(os.path.join(os.path.join(folder_out, 'pred'), name+'_COP30.png'), bbox_inches=extent8.expanded(1.3, 1.1))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    modelRSME = np.sqrt(mean_squared_error(narr_dtmAf, narr_dtmf))\n",
    "    fabdemRSME = np.sqrt(mean_squared_error(narr_fabdem, narr_dtmf[0:narr_fabdem.shape[0], 0:narr_fabdem.shape[1]]))\n",
    "    modelCRMSE = np.sqrt(mean_squared_error(narr_dtmfc, narr_dtmf[0:narr_dtmfc.shape[0], 0:narr_dtmfc.shape[1]]))\n",
    "\n",
    "    dt = {'modelRSME': modelRSME, 'fabdemRSME': fabdemRSME, 'modelCRMSE': modelCRMSE}\n",
    "\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:28<00:00,  2.96s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type float32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m pbar\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults_RMSE/RMSEs.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n\u001b[0;32m---> 16\u001b[0m     json\u001b[39m.\u001b[39;49mdump(dt_RMSE, outfile)\n",
      "File \u001b[0;32m~/mambaforge/envs/torG2/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/mambaforge/envs/torG2/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/torG2/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/torG2/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/torG2/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[1;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/torG2/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "dt_RMSE = {}\n",
    "pbar = tqdm(total=len(list_patch))\n",
    "for item in list_patch:\n",
    "    dt_RMSE[item]=rmseNMore(path_cop='/Users/hyde-mbp/Projects/Grad/data/COP30/', \n",
    "          path_ANH4=path_ANH4, \n",
    "          path_fabdem='/Users/hyde-mbp/Projects/Grad/data/FABDEM/', \n",
    "          name=item, \n",
    "          folder_out='results_RMSE')\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class NumpyFloatValuesEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "with open(\"results_RMSE/RMSEs.json\", \"w\") as outfile:\n",
    "    json.dump(dt_RMSE, outfile, cls=NumpyFloatValuesEncoder, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values: {'modelRSME': 0.6008902508020401, 'fabdemRSME': 0.9341578716039658, 'modelCRMSE': 1.1768759590387345}\n",
      "Min values: {'modelRSME': ('10EN2', 0.12665087), 'fabdemRSME': ('65GZ2', 0.37494177), 'modelCRMSE': ('10FZ2', 0.39659908)}\n",
      "Max values: {'modelRSME': ('25CN1', 1.7051184), 'fabdemRSME': ('25CN1', 2.9643419), 'modelCRMSE': ('49GN1', 3.7635415)}\n"
     ]
    }
   ],
   "source": [
    "min_values = {}\n",
    "max_values = {}\n",
    "sum_values = {key: 0 for key in dt_RMSE[next(iter(dt_RMSE))].keys()}\n",
    "\n",
    "# Iterate through the dictionary to find min, max, and sum values for each RMSE\n",
    "for key, sub_dict in dt_RMSE.items():\n",
    "    for sub_key, value in sub_dict.items():\n",
    "        if sub_key not in min_values or value < min_values[sub_key][1]:\n",
    "            min_values[sub_key] = (key, value)\n",
    "        if sub_key not in max_values or value > max_values[sub_key][1]:\n",
    "            max_values[sub_key] = (key, value)\n",
    "        sum_values[sub_key] += value\n",
    "\n",
    "# Calculate the mean values\n",
    "mean_values = {key: value / len(dt_RMSE) for key, value in sum_values.items()}\n",
    "\n",
    "print(\"Mean values:\", mean_values)\n",
    "print(\"Min values:\", min_values)\n",
    "print(\"Max values:\", max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plots are saved as result_other_areas/Austria_f4.png'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDtmNPlots(cop='/Users/hyde-mbp/Projects/Grad/data/COP30/Austria_re2.tiff',\n",
    "            fabdem='/Users/hyde-mbp/Projects/Grad/data/FABDEM/Austria_re2.tiff', \n",
    "            name='Austria', \n",
    "            folder_out='result_other_areas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torG2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
